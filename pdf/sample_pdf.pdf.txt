C/C++
Derek
Askaryar,
Mike
Zhang,
David
Thao
CS
149
CS149:
Written
Assignment
2
Questions:
https://gfxcourses.stanford.edu/cs149/fall24content/static/pdfs/written_asst2.pdf
Question
1
P a r t
A
(15
pts)
You
run
the
program
on
a
four
core
processor,
and
observe
that
it
gets
the
correct
answer,
and
that
work
is
well
distributed
among
the
threads.
However
you
don’t
observe
a
great
speedup
compared
to
a
single
threaded
version
of
the
code.
What
is
a
potential
significant
performance
problem?
The
performance
of
this
task
distribution
is
limited
by
the
resource
lock
imposed
on
the
bins
array,
which
limits
the
increment
operation
to
one
thread
at
a
time.
This
effectively
interleaves
the
threading
operations
and
will
not
be
significantly
better
than
simply
running
the
program
in
serial.
P a r t
B
(15
pts)
Imagine
that
instead
of
locks,
you
are
allowed
to
use
a
single
barrier()
in
the
code.
Please
give
a
solution
that
yields
good
work
distribution
onto
all
4
threads,
uses
no
locks,
and
uses
only
a
single
call
to
barrier().
Your
solution
is
allowed
to
allocate
new
global
or
per-thread
variables.
Hint:
Keep
in
mind
that
N
is
assumed
to
be
much,
much
larger
than
the
number
of
bins
in
the
histogram.
const
int
N
=
VERY_LARGE_NUMBER;
//
assume
N
is
a
very
large
number
const
int
NUM_THREADS
=
4;
int
data[N];
#
NEW_CODE:
per-thread
bins
int
bins_thread[NUM_THREADS][10];
int
bins[10];
//
assume
initialized
to
0
void
run(int
threadId)
{
int
elsPerThread
=
N
/
NUM_THREADS;
int
start
=
threadId
*
elsPerThread;
int
end
=
start
+
elsPerThread;
for
(int
i=start;
i<end;
i++)
{C/C++
int
binId
=
data[i]
/
10;
#
NEW
CODE:
have
threads
only
update
their
own
results
bins_thread[threadId][binId]
++;
}
#
NEW
CODE:
Ensure
all
threads
finish
before
adding
barrier()
#
NEW
CODE:
join
the
results
of
all
threads
if
(threadId
==
0)
{
for
(int
j
=
0;
j
<
NUM_THREADS;
j++)
{
for
(int
k
=
0;
k
<
10;
k++)
{
bins[k]
+=
bins_thread[j][k]
}
}
}
}
Question
2
P a r t
A
(15
pts)
Although
the
code
makes
N2/2
calls
to
gravity()
it
takes
N2
locks.
Modify
the
code
so
that
the
number
of
lock/unlock
operations
is
reduced
by
2×.
You
may
not
allocate
additional
variables
or
change
how
loop
iterations
are
mapped
to
the
threads.
struct
Particle
{
float
force;
//
for
simplicity,
assume
force
is
represented
as
a
single
float
Lock
l;
};
Particle
particles[N];
void
compute_forces(
int
threadId)
{
//
thread
0
takes
first
half,
thread
1
takes
second
half
int
start
=
threadId
*
N/
2
;
int
end
=
start
+
N/
2
;
for
(
int
i=start;
i<end;
i++)
{
//
only
compute
forces
for
each
pair
(i,j)
once,
then
accumulate
forceC/C++
//
into
*both*
particle
i
and
j
//
NEW
CODE:
lock
the
ith
particle
here
lock(particles[i].l);
for
(
int
j=i+
1
;
j<N;
j++)
{
float
force
=
gravity(i,
j);
particles[i]
+=
force;
lock(particles[j].l);
particles[j]
+=
force;
unlock(particles[j].l);
}
//
NEW
CODE:
unlock
once
unlock(particles[i].l);
}
}
P a r t
B
(15
pts)
(This
question
can
be
answered
independently
from
part
A)
Looking
at
the
original
code,
there
another
major
performance
problem
that
does
not
have
to
do
with
the
number
of
lock/unlock
operations.
Please
describe
the
problem
and
then
describe
a
solution.
Clearly
describing
an
implementable
solution
strategy
is
fine,
you
do
not
need
to
write
precise
pseudocode.
The
work
distribution
is
highly
uneven
between
the
two
threads.
We
see
that
the
inner
loop
runs
from
j
=
i
…
j
=
N.
Thread
1
begins
at
i
=
0…N/2
while
Thread
2
begins
at
i
=
N
/
2,
meaning
that
the
inner
loop
for
Thread
1
is
clearly
doing
more
work
than
Thread
2.
Specifically,
the
first
run
of
the
inner
loop
will
run
from
j
=
0…N
in
Thread
1
while
only
running
from
j
=
N/2…N
in
Thread
2.
We
modify
the
code
to
run
in
this
way:
both
threads
run
the
outer
loop
from
i
=
0…N.
In
the
inner
loop,
thread
1
computes
even
values
of
j
=
0,
2,
4,
…
while
thread
2
computes
odd
values
of
j
=
1,
3,
5,
…
This
ensures
an
even
division
of
work.
struct
Particle
{
float
force;
//
for
simplicity,
assume
force
is
represented
as
a
single
float
Lock
l;
};
Particle
particles[N];
void
compute_forces(
int
threadId)
{#
NEW
CODE:
thread
1
does
all
even,
2
does
all
odd
for
(
int
i=threadId;
i<N;
i
+=
2
)
{
lock(particles[i].l);
for
(
int
j=i+
1
;
j<N;
j++)
{
float
force
=
gravity(i,
j);
particles[i]
+=
force;
lock(particles[j].l);
particles[j]
+=
force;
unlock(particles[j].l);
}
unlock(particles[i].l);
}
}
–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––
Question
3
P a r t
A
(10
pts)
A
key
idea
in
this
course
is
the
difference
between
abstraction
and
implementation.
Consider
two
abstractions
we’ve
studied:
ISPC’s
foreach
and
Cilk’s
cilk_spawn
construct.
Briefly
describe
how
these
two
abstractions
have
similar
semantics.
(Hint:
what
do
the
constructs
declare
about
the
associated
loop
iterations?
Be
precise!).
Then
briefly
describe
how
their
implementations
are
quite
different
(Hint:
consider
their
mapping
to
execution
contexts
and
SIMD
vector
instructions
on
modern
CPUs).
As
a
reminder,
we
give
you
two
syntax
examples
below:
Both
ISPC’s
foreach
and
Cilk’s
cilk_spawn
effectively
declare
that
each
iteration
of
the
loop
are
independent
of
each
other
and
don’t
depend
on
the
same
data,
so
they
can
be
computed
out
of
order
or
in
parallel
and
still
compute
the
same
result
in
the
end.
C/C++
In
the
case
of
ISPC
foreach,
it
is
not
creating
a
new
thread
of
control
or
program
instance,
but
rather
uses
a
gang
of
workers
to
execute
iterations
of
the
loop
in
parallel
using
SIMD.
This
is
possible
because
we
are
executing
a
single
instruction
(x[i]=y[i]),
but
over
multiple
data
(SIMD).
Cilk
creates
an
entirely
new
execution
context
using
a
thread
pool
for
the
function
that
it
is
deploying,
and
is
therefore
more
resource
intensive..
P a r t
B
(10
pts)
When
we
discussed
Cilk,
we
emphasized
how
cilk_spawn
foo()
differs
from
a
normal
C
function
call
foo()
in
that
the
Cilk
call
can
run
asynchronously
with
the
caller.
Notice
that
Cilk
doesn’t
explicitly
state
that
the
callee
function
runs
in
parallel
with
the
caller.
Give
one
reason
why
the
designers
of
Cilk
intentionally
designed
a
language
that
does
not
specify
when
the
call
will
run
relative
to
the
caller?
The
ability
to
schedule
tasks
out
of
order
enables
different
work
assignment
policies
for
parallel
computing.
If
we
were
to
specify
an
order
of
operation,
we
could
achieve
suboptimal
speedups
if
some
of
the
calls
take
a
long
time
(i.e.
if
we
ran
tasks
A,
B,
C,
D
on
a
dual-core
processor,
where
task
B
and
D
take
a
long
time,
running
them
in
ABCD
may
be
less
efficient
than
allocating
A
and
B
to
a
processor
and
C
and
D
to
the
second
one).
P a r t
C
(10
pts)
Consider
a
cache
that
contains
32
KB
of
data,
has
a
cache
line
size
of
4
bytes,
is
fully
associative
(meaning
any
cache
line
can
go
anywhere
in
the
cache),
and
uses
an
LRU
(least
recently
used—the
line
evicted
is
the
line
that
was
last
accessed
the
longest
time
ago)
replacement
policy.
Please
describe
why
the
following
code
will
take
a
cache
miss
on
every
data
access
to
the
array
A.
const
int
SIZE
=
1024
*
64
;
float
A[SIZE];
float
sum
=
0
.
0
;
for
(
int
reps=
0
;
reps<
32
;
reps++)
for
(
int
i=
0
,
i<SIZE;
i++)
sum
+=
A[i];
The
array
A
has
the
capacity
of
1024*64
floats
and
each
float
is
4
bytes,
so
it
takes
up
a
total
of
about
262
KB.
This
is
significantly
larger
than
the
cache.
The
inner
loop
loops
over
every
element
in
the
array
A
and
since
only
32KB
of
A
can
fit
in
the
cache
at
once,
when
one
iteration
of
the
outer
loop
is
done
the
cache
is
only
going
to
hold
the
last
32KB
of
Ax.
Then,
when
the
next
iteration
of
the
outer
loop
starts,
the
inner
loop
is
going
to
have
to
recall
each
element
again
from
the
memory
since
it
starts
from
the
beginning
of
A
so
there
will
only
be
cache
misses.P a r t
D
(10
pts)
Your
friend
is
designing
a
soccer
playing
robot
for
the
next
RoboCup
competition.
The
software
on
the
robot
must
send
a
torque
request
to
the
robot’s
motors
every
3
ms
in
order
for
the
robot
to
successfully
locomote.
Unfortunately
in
your
friend’s
implementation,
computing
torques
takes
20
ms
when
running
serially
on
a
single
core
of
the
robot’s
2
GHz
single-core
CPU.
As
a
result
your
friend’s
robot
constantly
falls
over
without
being
touched.
You
laugh
at
your
friend,
and
call
their
robot
“Neymar“!
You
look
at
your
friend’s
code
and
notice
that
20%
of
the
code
is
inherently
serial.
The
rest
is
perfectly
parallelizable.
You
dig
into
your
desk
drawer
and
find
two
processors:
Processor
A
is
a
64-core
processor
that
runs
at
1
GHz.
Processor
B
is
a
8-core
processor
that
runs
at
4
GHz.
Can
you
solve
your
friend’s
performance
problem?
Justify
your
answer
by
computing
the
maximum
performance
they
can
achieve.
Processor
B
would
provide
the
maximum
performance,
since
in
doing
serial
work,
Processor
B
outperforms
Processor
A.
In
doing
parallel
work,
Processor
B
matches
the
performance
of
Processor
A.
When
calculating
the
the
time
it
takes
for
Processor
B
to
compute
the
torques,
the
serial
work
would
take
2ms,
since
we’ve
doubled
the
clock
rate
of
the
processor,
while
the
parallel
work
would
take
1ms
as
we
have
8
cores
running
at
double
the
clock
rate,
resulting
in
16x
faster
computation.